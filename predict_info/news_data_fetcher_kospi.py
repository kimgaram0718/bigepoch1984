from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import time, os, re, sys
from datetime import datetime, timedelta
from tqdm import tqdm
import FinanceDataReader as fdr
from time import perf_counter

# ì„¤ì •
CHROMEDRIVER_PATH = "/usr/bin/chromedriver"
SAVE_DIR = "news_data/chosun"
os.makedirs(SAVE_DIR, exist_ok=True)
START_DATE = datetime(2020, 1, 1)
END_DATE = datetime.today() - timedelta(days=1)

# ì™„ë£Œëœ ì¢…ëª© ì²´í¬
def get_completed_stocks(file_path):
    if not os.path.exists(file_path):
        return set()
    df = pd.read_csv(file_path)
    return set(df['ì¢…ëª©ëª…'].unique())

# ë‚ ì§œ ì¶”ì¶œ
def extract_date(text):
    match = re.search(r"\d{4}\.\d{2}\.\d{2}", text)
    if match:
        try:
            return datetime.strptime(match.group(0), "%Y.%m.%d")
        except:
            return None
    return None

#í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
def get_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("user-agent=Mozilla/5.0")
    options.add_argument("--disable-gpu")
    options.add_argument("--blink-settings=imagesEnabled=false")
    
    return webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=options)
    
# ì„œë¸Œë„ë©”ì¸ êµ¬ë¶„ ë° íŒŒì‹±
def parse_chosun(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    paragraphs = soup.select("p.article-body__content-text")
    return " ".join(p.get_text(strip=True) for p in paragraphs)

def parse_biz(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    paragraphs = soup.select("p.article-body__content.article-body__content-text")
    return " ".join(p.get_text(strip=True) for p in paragraphs)

def parse_it(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    body = soup.select_one("div.article-body")
    return " ".join(p.get_text(strip=True) for p in body.find_all("p")) if body else ""

def parse_tv(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    box = soup.select_one("div.text-box")
    return " ".join(p.get_text(strip=True) for p in box.find_all("p")) if box else ""

def get_parser_by_url(url):
    if "biz.chosun.com" in url:
        return parse_biz, "ë¹„ì¦ˆì¡°ì„ "
    elif "it.chosun.com" in url:
        return parse_it, "ITì¡°ì„ "
    elif "tvchosun.com" in url:
        return parse_tv, "TVì¡°ì„ "
    else:
        return parse_chosun, "ì¡°ì„ ì¼ë³´"

# ê¸°ì‚¬ í¬ë¡¤ë§
def crawl_chosun_news(keyword):
    driver = get_driver()
    page, print_count, empty_count = 1, 0, 0
    results, seen_links = [], set()

    while True:
        query_url = f"https://www.chosun.com/nsearch/?query={keyword}&pageno={page}"
        driver.get(query_url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        cards = soup.select("div.story-card")
        if not cards:
            break

        total_cards = len(cards)
        added, excluded = 0, 0
        valid = False

        for card in cards:
            title_tag = card.select_one("a.text__link.story-card__headline")
            summary_tag = card.select_one("span.story-card__deck") or card.select_one("span[style]")
            breadcrumb_tag = card.select_one("div.story-card__breadcrumb")

            if not title_tag:
                excluded += 1
                continue

            title = title_tag.get_text(strip=True)
            link = title_tag["href"]
            if link.startswith("/"):
                link = "https://www.chosun.com" + link
            if link in seen_links or not any(d in link for d in ["chosun.com"]):
                excluded += 1
                continue
            seen_links.add(link)

            summary = summary_tag.get_text(strip=True) if summary_tag else ""
            breadcrumb_text = breadcrumb_tag.get_text(" ", strip=True) if breadcrumb_tag else ""
            date = extract_date(breadcrumb_text)
            if not date or date < START_DATE or date > END_DATE:
                continue
            valid = True

            try:
                driver.get(link)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                parser, press = get_parser_by_url(link)
                content = parser(driver)
            except:
                continue

            results.append({
                "ì¢…ëª©ëª…": keyword,
                "ë‚ ì§œ": date.strftime("%Y-%m-%d"),
                "ì œëª©": title,
                "ìš”ì•½": summary,
                "ë³¸ë¬¸": content,
                "URL": link,
                "ì–¸ë¡ ì‚¬": press
            })
            added += 1

        tqdm.write(
            f"[ğŸ” {keyword}] Page {page} | ê¸°ì‚¬ ìˆ˜: {total_cards}, ì œì™¸: {excluded}, ìˆ˜ì§‘: {added}, ëˆ„ì : {print_count}"
        )
        print_count += added

        if not valid:
            empty_count_in_page += 1
        else:
            empty_count_in_page = 0

        if empty_count_in_page >= 200:
            tqdm.write(f"[ì¤‘ë‹¨] {keyword} - ìµœê·¼ ê¸°ì‚¬ ì—†ìŒ (200í˜ì´ì§€ ì—°ì†)")
            break

        page += 1

    driver.quit()
    return results, print_count

# ì‹¤í–‰ë¶€
if __name__ == "__main__":
    start_time = perf_counter()

    print("FDRì—ì„œ ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì¢…ëª© ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    kospi_list = fdr.StockListing('KOSPI')['Name'].dropna().unique().tolist()

    completed_kospi = get_completed_stocks(os.path.join(SAVE_DIR, "chosun_kospi_articles.csv"))

    kospi_list = [s for s in kospi_list if s not in completed_kospi]
    print(f"â†’ ì´ì–´ë°›ê¸° ì ìš©: KOSPI {len(kospi_list)}ê°œ")

    combined_list = [("KOSPI", stock) for stock in kospi_list]
    
    all_kospi = []
    total_kospi = 0

    def threaded_crawl(args):
        market, stock = args
        try:
            articles, count = crawl_chosun_news(stock)
            return (market, stock, articles, count)
        except Exception as e:
            tqdm.write(f"[âš ] ì‹¤íŒ¨: {stock} ({e})")
            return (market, stock, [], 0)

    try:
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(threaded_crawl, arg) for arg in combined_list]
            for future in tqdm(as_completed(futures), total=len(futures), desc="ì „ì²´ ì§„í–‰ë¥ ", ncols=100, dynamic_ncols=True):
                market, stock, articles, count = future.result()
                if market == "KOSPI":
                    all_kospi.extend(articles)
                    total_kospi += count
                else:
                    continue
                tqdm.write(f"[âœ”] {stock} ìˆ˜ì§‘ ì™„ë£Œ â–¶ ì¢…ëª© ëˆ„ì : {count}, KOSPI ì´: {total_kospi}")

    except KeyboardInterrupt:
        print("\nâ›” [ì¤‘ë‹¨] ì‚¬ìš©ì ìˆ˜ë™ ì¢…ë£Œ")

    finally:
        cols = ["ì¢…ëª©ëª…", "ë‚ ì§œ", "ì œëª©", "ìš”ì•½", "ë³¸ë¬¸", "URL", "ì–¸ë¡ ì‚¬"]
        if all_kospi:
            pd.DataFrame(all_kospi)[cols].to_csv(os.path.join(SAVE_DIR, "chosun_kospi_articles.csv"), index=False, encoding="utf-8-sig")
            print(f"KOSPI ì €ì¥ ì™„ë£Œ: {len(all_kospi)}ê±´")
        elapsed = perf_counter() - start_time
        print(f"\nâœ… ì „ì²´ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ | ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ â‰ˆ {elapsed/60:.1f}ë¶„")